import torch
import torch.nn as nn
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import os
import pickle
import io
import re
import sys

if getattr(sys, 'frozen', False):
    BASE_DIR = sys._MEIPASS
else:
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# ƒê·∫£m b·∫£o d·ªØ li·ªáu NLTK ƒë∆∞·ª£c c√†i ƒë·∫∑t
if not os.path.exists("nltk_data"):
    print("[INFO] T·∫°o th∆∞ m·ª•c nltk_data v√† t·∫£i d·ªØ li·ªáu NLTK...")
    os.makedirs("nltk_data")
    nltk.download('punkt', download_dir="nltk_data")
    nltk.download('stopwords', download_dir="nltk_data")
try:
    nltk.data.path.append("nltk_data")
except:
    print("[WARNING] Kh√¥ng th·ªÉ th√™m nltk_data v√†o ƒë∆∞·ªùng d·∫´n")

# ============== ƒê·ªäNH NGHƒ®A L·ªöP VOCABULARY D√ôNG TRONG HU·∫§N LUY·ªÜN ================
# ƒê·ªãnh nghƒ©a l·ªõp Vocabulary ·ªü c·∫•p ƒë·ªô module ch√≠nh ƒë·ªÉ PyTorch c√≥ th·ªÉ t√¨m th·∫•y
class Vocabulary:
    def __init__(self, max_size=50000):
        self.max_size = max_size
        self.token2idx = {'<pad>': 0, '<unk>': 1}
        self.idx2token = {0: '<pad>', 1: '<unk>'}
        
    def __len__(self):
        return len(self.token2idx)
        
    def __getitem__(self, token):
        return self.token2idx.get(token, 1)  # Tr·∫£ v·ªÅ index c·ªßa token, n·∫øu kh√¥ng c√≥ th√¨ tr·∫£ v·ªÅ index c·ªßa <unk>

    def build_vocab(self, texts, min_freq=1):
        """X√¢y d·ª±ng t·ª´ ƒëi·ªÉn t·ª´ danh s√°ch vƒÉn b·∫£n"""
        freq = {}
        for text in texts:
            for token in text:
                if token in freq:
                    freq[token] += 1
                else:
                    freq[token] = 1
        
        # S·∫Øp x·∫øp theo t·∫ßn su·∫•t gi·∫£m d·∫ßn
        sorted_tokens = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        
        # L·∫•y nh·ªØng token c√≥ t·∫ßn su·∫•t >= min_freq
        idx = 2  # B·∫Øt ƒë·∫ßu t·ª´ 2 v√¨ 0, 1 ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng
        for token, count in sorted_tokens:
            if count >= min_freq and idx < self.max_size:
                self.token2idx[token] = idx
                self.idx2token[idx] = token
                idx += 1
                
    def numericalize(self, text):
        """Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh d√£y s·ªë"""
        return [self.__getitem__(token) for token in text]

# Adapter cho Vocabulary ƒë·ªÉ x·ª≠ l√Ω nhi·ªÅu c·∫•u tr√∫c kh√°c nhau
class VocabularyAdapter:
    def __init__(self, vocab_object):
        self.vocab_object = vocab_object
        # X√°c ƒë·ªãnh lo·∫°i c·ªßa vocab object
        if hasattr(vocab_object, 'token2idx'):
            # ƒê√¢y l√† Vocabulary class
            self.vocab_type = 'vocab_class'
        elif isinstance(vocab_object, dict) and '<pad>' in vocab_object:
            # ƒê√¢y l√† dictionary ƒë∆°n gi·∫£n token->idx
            self.vocab_type = 'dict'
        else:
            # Lo·∫°i kh√¥ng x√°c ƒë·ªãnh
            print(f"[WARNING] Kh√¥ng th·ªÉ x√°c ƒë·ªãnh lo·∫°i vocabulary: {type(vocab_object)}")
            self.vocab_type = 'unknown'
            
    def __getitem__(self, token):
        if self.vocab_type == 'vocab_class':
            return self.vocab_object[token]
        elif self.vocab_type == 'dict':
            return self.vocab_object.get(token, self.vocab_object.get('<unk>', 1))
        else:
            return 1  # Tr·∫£ v·ªÅ <unk> cho tr∆∞·ªùng h·ª£p kh√¥ng x√°c ƒë·ªãnh
            
    def __len__(self):
        if self.vocab_type == 'vocab_class':
            return len(self.vocab_object)
        elif self.vocab_type == 'dict':
            return len(self.vocab_object)
        else:
            return 50000  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh

# ============== ƒê·ªäNH NGHƒ®A L·ªöP MODEL D√ôNG TRONG HU·∫§N LUY·ªÜN ================
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout, bidirectional=True):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, 
                          dropout=dropout if num_layers > 1 else 0, bidirectional=bidirectional)
        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, text, text_lengths=None):
        embedded = self.dropout(self.embedding(text))
        
        if text_lengths is not None:
            # N·∫øu c√≥ text_lengths, s·ª≠ d·ª•ng pack_padded_sequence
            packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), 
                                                             batch_first=True, enforce_sorted=False)
            packed_output, (hidden, cell) = self.lstm(packed_embedded)
        else:
            # N·∫øu kh√¥ng c√≥ text_lengths (nh∆∞ trong d·ª± ƒëo√°n), s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p thay th·∫ø
            output, (hidden, cell) = self.lstm(embedded)
        
        if self.lstm.bidirectional:
            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        else:
            hidden = hidden[-1,:,:]
            
        hidden = self.dropout(hidden)
        output = self.fc(hidden)
        
        return output

# H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n - c·∫£i ti·∫øn ph√π h·ª£p v·ªõi h√†m d√πng ƒë·ªÉ train
def preprocess_text(text):
    """H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: lo·∫°i b·ªè stopwords, bi·∫øn ƒë·ªïi v·ªÅ ch·ªØ th∆∞·ªùng..."""
    if not isinstance(text, str):
        return []
    
    # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    text = text.lower()
    
    # Lo·∫°i b·ªè c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát, gi·ªØ l·∫°i ch·ªØ c√°i v√† d·∫•u c√¢u quan tr·ªçng
    text = re.sub(r'[^a-zA-Z\s.,!?]', '', text)
    # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Tokenize
    tokens = word_tokenize(text)
    
    # Lo·∫°i b·ªè stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    
    return tokens

# H√†m t·∫°o t·ª´ ƒëi·ªÉn m·ªõi khi kh√¥ng th·ªÉ load t·ª´ ƒëi·ªÉn t·ª´ model
def create_vocab_from_scratch():
    """T·∫°o t·ª´ ƒëi·ªÉn c∆° b·∫£n cho model khi kh√¥ng load ƒë∆∞·ª£c t·ª´ ƒëi·ªÉn t·ª´ model"""
    vocab = Vocabulary(max_size=50000)
    
    # Danh s√°ch t·ª´ ph·ªï bi·∫øn trong c√°c email
    common_words = ["email", "dear", "hello", "hi", "please", "account", "bank", "password", 
                    "verify", "click", "link", "urgent", "money", "transfer", "security", 
                    "update", "information", "confirm", "payment", "credit", "card", "login",
                    "access", "online", "service", "customer", "support", "team", "help",
                    "request", "attention", "important", "notification", "alert", "warning",
                    "suspicious", "activity", "detected", "verify", "verification", "identity",
                    "secure", "protect", "fraud", "unauthorized", "transaction", "account",
                    "balance", "statement", "bill", "invoice", "receipt", "due", "date",
                    # Th√™m t·ª´ v·ª±ng ph·ªï bi·∫øn trong email l·ª´a ƒë·∫£o
                    "limited", "offer", "free", "gift", "prize", "winner", "won", "million",
                    "dollar", "cash", "paypal", "discount", "sale", "virus", "antivirus",
                    "lottery", "jackpot", "customer", "service", "refund", "claim", "bonus"]
    
    # Th√™m v√†o t·ª´ ƒëi·ªÉn
    for idx, word in enumerate(common_words, 2):  # B·∫Øt ƒë·∫ßu t·ª´ 2 v√¨ 0, 1 ƒë√£ s·ª≠ d·ª•ng
        if word not in vocab.token2idx:
            vocab.token2idx[word] = idx
            vocab.idx2token[idx] = word
    
    return vocab

# ============== L·ªöP SPAM DETECTOR ================
class SpamDetector:
    _instance = None  # Bi·∫øn tƒ©nh ƒë·ªÉ l∆∞u tr·ªØ instance duy nh·∫•t
    
    @classmethod
    def get_instance(cls, model_path='models/best_model.pth'):
        """Tri·ªÉn khai Singleton pattern ƒë·ªÉ tr√°nh t·∫£i model nhi·ªÅu l·∫ßn"""
        if cls._instance is None:
            cls._instance = cls(model_path)
        return cls._instance
    
    def __init__(self, model_path='models/best_model.pth', max_len=200):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.max_len = max_len
        self.model = None
        self.vocab = None
        
        try:
            # ƒêƒÉng k√Ω Vocabulary class v·ªõi global namespace
            try:
                sys.modules['__main__'].Vocabulary = Vocabulary
            except:
                pass
            
            # T√¨m model ·ªü c√°c v·ªã tr√≠ kh√°c nhau
            possible_paths = [
                model_path,
                os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'models', 'best_model.pth'),
                os.path.join(os.getcwd(), 'models', 'best_model.pth')
            ]
            
            model_file = None
            for path in possible_paths:
                if os.path.exists(path):
                    model_file = path
                    break
            
            if not model_file:
                print("[WARNING] Kh√¥ng t√¨m th·∫•y model, s·∫Ω s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng")
                return
                
            # T·∫£i checkpoint
            checkpoint = None
            try:
                checkpoint = torch.load(model_file, map_location=self.device, weights_only=False)
            except:
                checkpoint = torch.load(model_file, map_location=self.device)
            
            if checkpoint is None:
                return
                
            # L·∫•y vocabulary t·ª´ checkpoint ho·∫∑c t·∫°o m·ªõi
            if isinstance(checkpoint, dict) and 'vocab' in checkpoint:
                self.vocab = VocabularyAdapter(checkpoint['vocab'])
            else:
                self.vocab = VocabularyAdapter(create_vocab_from_scratch())
            
            vocab_size = len(self.vocab)
            
            # Kh·ªüi t·∫°o model v·ªõi tham s·ªë
            embedding_dim = checkpoint.get('embedding_dim', 300) if isinstance(checkpoint, dict) else 300
            hidden_dim = checkpoint.get('hidden_dim', 256) if isinstance(checkpoint, dict) else 256
            num_layers = checkpoint.get('num_layers', 2) if isinstance(checkpoint, dict) else 2
            output_dim = checkpoint.get('output_dim', 1) if isinstance(checkpoint, dict) else 1
            dropout = checkpoint.get('dropout', 0.5) if isinstance(checkpoint, dict) else 0.5
            bidirectional = checkpoint.get('bidirectional', True) if isinstance(checkpoint, dict) else True
            
            self.model = LSTMModel(
                vocab_size=vocab_size,
                embedding_dim=embedding_dim,
                hidden_dim=hidden_dim,
                num_layers=num_layers,
                output_dim=output_dim,
                dropout=dropout,
                bidirectional=bidirectional
            )
            
            # T·∫£i tr·ªçng s·ªë
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                try:
                    self.model.load_state_dict(checkpoint)
                except:
                    print("[WARNING] Kh√¥ng th·ªÉ t·∫£i tr·ªçng s·ªë model")
                    
            # Chuy·ªÉn model sang device v√† ƒë·∫∑t ·ªü ch·∫ø ƒë·ªô evaluation
            self.model.to(self.device)
            self.model.eval()
            
        except Exception as e:
            print(f"[ERROR] L·ªói khi t·∫£i model: {e}")
            self.model = None
            self.vocab = None
    
    def predict(self, text):
        """D·ª± ƒëo√°n xem email c√≥ ph·∫£i l√† spam hay kh√¥ng"""
        # N·∫øu kh√¥ng c√≥ model, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p fallback
        if self.model is None or self.vocab is None:
            return self.keyword_based_prediction(text)
            
        try:
            # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n
            processed_text = preprocess_text(text)
            
            # Ki·ªÉm tra xem vƒÉn b·∫£n c√≥ r·ªóng kh√¥ng
            if not processed_text:
                print("[WARNING] VƒÉn b·∫£n r·ªóng sau khi ti·ªÅn x·ª≠ l√Ω")
                return 0.1  # Tr·∫£ v·ªÅ x√°c su·∫•t th·∫•p cho vƒÉn b·∫£n r·ªóng
            
            # Chuy·ªÉn tokens th√†nh indices
            indices = [self.vocab[token] for token in processed_text]
            
            # C·∫Øt ho·∫∑c padding ƒë·ªÉ ƒë·∫°t max_len
            if len(indices) < self.max_len:
                indices = indices + [0] * (self.max_len - len(indices))  # <pad> token c√≥ index l√† 0
            else:
                indices = indices[:self.max_len]
            
            # Chuy·ªÉn sang tensor
            text_tensor = torch.LongTensor([indices]).to(self.device)
            length_tensor = torch.LongTensor([min(len(processed_text), self.max_len)]).to(self.device)
            
            # D·ª± ƒëo√°n
            with torch.no_grad():
                logits = self.model(text_tensor, length_tensor)
                # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p logits l√† float
                if isinstance(logits, float):
                    prediction = logits
                else:
                    logits = logits.squeeze()
                    prediction = torch.sigmoid(logits).item()
                    
                # √Åp d·ª•ng quy t·∫Øc ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c:
                if 0.4 <= prediction <= 0.6:
                    keyword_score = self.keyword_based_prediction(text)
                    if keyword_score > 0.7:
                        prediction = max(prediction, (prediction + keyword_score) / 2)
                    elif keyword_score < 0.3:
                        prediction = min(prediction, (prediction + keyword_score) / 2)
            
            print(f"[INFO] D·ª± ƒëo√°n th√†nh c√¥ng v·ªõi m√¥ h√¨nh AI. ƒêi·ªÉm spam: {prediction:.4f}")
            return prediction  # Tr·∫£ v·ªÅ x√°c su·∫•t l√† spam (0-1)
        except Exception as e:
            print(f"[ERROR] L·ªói khi d·ª± ƒëo√°n: {e}")
            import traceback
            traceback.print_exc()
            # N·∫øu c√≥ l·ªói, s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª± ph√≤ng d·ª±a tr√™n t·ª´ kh√≥a
            return self.keyword_based_prediction(text)
    
    def keyword_based_prediction(self, text):
        """Ph√¢n t√≠ch d·ª±a tr√™n t·ª´ kh√≥a khi model ML kh√¥ng kh·∫£ d·ª•ng"""
        if not text or not isinstance(text, str):
            return 0.0
            
        # Chuy·ªÉn text v·ªÅ ch·ªØ th∆∞·ªùng ƒë·ªÉ d·ªÖ so s√°nh
        text = text.lower()
        
        # Danh s√°ch c√°c t·ª´ kh√≥a ph·ªï bi·∫øn trong email l·ª´a ƒë·∫£o/spam - C·∫¢I TI·∫æN DANH S√ÅCH
        spam_keywords = [
            # T·ª´ kh√≥a li√™n quan ƒë·∫øn t√≠nh kh·∫©n c·∫•p
            'urgent', 'immediate action', 'act now', 'limited time', 'deadline', 'expiration',
            'last chance', 'don\'t delay', 'hurry', 'kh·∫©n c·∫•p', 'kh√¥ng ƒë∆∞·ª£c b·ªè l·ª°',
            
            # Ti·ªÅn b·∫°c v√† c∆° h·ªôi
            'lottery', 'winner', 'free money', 'million dollar', 'prize', 'jackpot', 'cash', 
            'investment opportunity', 'nigeria', 'inheritance', 'prince', 'loan', 'profit',
            'claim your prize', 'bank transfer', 'offshore', 'opportunity', 'fortune',
            'x·ªï s·ªë', 'tr√∫ng th∆∞·ªüng', 'ti·ªÅn t·ª∑', 'b·∫°n ƒë√£ tr√∫ng', 'c∆° h·ªôi ƒë·∫ßu t∆∞',
            
            # ∆Øu ƒë√£i v√† khuy·∫øn m√£i ƒë√°ng ng·ªù
            'congratulations', 'click here', 'offer', 'free', 'guaranteed', 'no risk',
            'best price', 'cash bonus', 'discount', 'special offer', 'exclusive deal',
            'limited offer', 'lifetime opportunity', 'free gift', 'premium', 'no cost',
            
            # Y t·∫ø v√† gi·∫£m c√¢n
            'cialis', 'viagra', 'weight loss', 'lose weight', 'miracle cure', 'no medical exams',
            'medicine', 'drug', 'pharmacy', 'prescription', 'diet', 'fat', 'slim',
            
            # T√†i ch√≠nh
            'earn extra cash', 'eliminate debt', 'extra income', 'fast cash', 'for free', 
            'for just $', 'double your money', 'financial freedom', 'get rich quick',
            'debt free', 'cash advance', 'payday loan', 'installment', 'refinance',
            'for only', 'from home', 'g·ª≠i ti·ªÅn', 'hidden assets', 'incredible deal',
            'money back', 'order now', 'please help', 'potential earnings', 'pure profit', 
            'risk-free', 'special promotion', 'supplies limited', 'take action now',
            'mi·ªÖn ph√≠', 'h√†nh ƒë·ªông ngay', 'c∆° h·ªôi cu·ªëi c√πng', 'thanh to√°n',
            
            # B·∫£o m·∫≠t v√† t√†i kho·∫£n ng√¢n h√†ng
            'verify', 'identity', 'suspicious', 'account', 'security', 'password', 
            'verify', 'bank', 'secure', 'unauthorized', 'access', 'credit card', 
            'authenticate', 'confirm', 'verification', 'validate', 'authorization',
            'b·∫£o m·∫≠t', 'ng√¢n h√†ng qu·ªëc t·∫ø', 'th·∫ª t√≠n d·ª•ng',
            'account blocked', 'suspended', 'verify identity', 'security alert',
            'password expired', 'login information', 'social security', 'update account',
            'suspicious activity', 'login details', 'personal details', 'credentials',
            
            # L√†m vi·ªác t·∫°i nh√†
            'work from home', 'no experience', 'guaranteed income', 'be your own boss',
            'make money online', 'earn money fast', 'income opportunity', 'residual income',
            
            # C√°c d·∫•u hi·ªáu l·ª´a ƒë·∫£o kh√°c
            'this is not spam', 'not spam', 'removed at any time', 'removal instructions', 
            'to be removed', 'to unsubscribe', 'this is not a scam', 'no scam', 'no spam', 
            'legitimate', 'this is legitimate', 'real thing', 'serious offer', 'serious business',
            'direct marketing', 'direct email', 'bulk email', 'mass email', 'opt in'
        ]
        
        # ƒê·∫øm s·ªë l∆∞·ª£ng t·ª´ kh√≥a xu·∫•t hi·ªán v√† tr·ªçng s·ªë cho t·ª´ng lo·∫°i
        keyword_count = 0
        urgent_count = 0
        money_count = 0
        security_count = 0
        sensitive_info_count = 0
        matched_keywords = []

        # Ph√¢n t√≠ch ng·ªØ c·∫£nh s·ª≠ d·ª•ng t·ª´ kh√≥a
        for keyword in spam_keywords:
            if keyword in text:
                matched_keywords.append(keyword)
                keyword_count += 1

                if keyword in ['urgent', 'immediate action', 'act now', 'deadline', 'expiration', 'kh·∫©n c·∫•p']:
                    urgent_count += 1
                elif keyword in ['money', 'cash', 'bank', 'transfer', 'million', 'dollar', 'payment']:
                    money_count += 1
                elif keyword in ['security', 'secure', 'verify', 'validate', 'authenticate', 'b·∫£o m·∫≠t']:
                    security_count += 1
                elif keyword in ['password', 'credit card', 'account number', 'login', 'social security', 'th·∫ª t√≠n d·ª•ng']:
                    sensitive_info_count += 1

        score = min(0.15 * keyword_count, 0.75)

        # C√°c y·∫øu t·ªë kh√°c
        exclamation_count = text.count('!')
        if exclamation_count > 3:
            score += min(0.05 * (exclamation_count - 3), 0.15)

        words = text.split()
        uppercase_words = sum(1 for word in words if word.isupper() and len(word) > 2)
        if uppercase_words > 3:
            score += min(0.05 * (uppercase_words - 3), 0.2)

        if '$' in text or '‚Ç¨' in text or '¬£' in text or 'ti·ªÅn' in text or 'ƒë√¥ la' in text:
            score += 0.15

        url_count = text.count('http') + text.count('www') + text.count('.com') + text.count('.net')
        if url_count > 1:
            score += min(0.1 * url_count, 0.25)

        suspicious_patterns = ['click here', 'click this link', 'nh·∫•p v√†o ƒë√¢y', 'b·∫•m v√†o li√™n k·∫øt']
        for pattern in suspicious_patterns:
            if pattern in text.lower():
                score += 0.15
                break

        if len(text) < 100:
            score *= (len(text) / 100)

        if urgent_count > 0:
            score += min(0.2 * urgent_count, 0.4)

        if sensitive_info_count > 0:
            score += min(0.25 * sensitive_info_count, 0.5)

        if urgent_count > 0 and sensitive_info_count > 0:
            score += 0.2

        if money_count > 0 and sensitive_info_count > 0:
            score += 0.2

        sentences = re.split(r'[.!?]', text)
        short_sentences = sum(1 for s in sentences if len(s.strip()) < 5)
        if short_sentences > len(sentences) * 0.5:
            score += 0.1

        score = min(score, 0.98)

        # üî• Tr·∫£ v·ªÅ c·∫£ score v√† danh s√°ch t·ª´ kh√≥a
        return score, matched_keywords


# H√†m l·∫•y detector ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u (singleton)
def get_spam_detector():
    return SpamDetector.get_instance()

# L·ªõp fallback ƒë∆°n gi·∫£n khi c·∫ßn thi·∫øt
class FallbackSpamDetector:
    def predict(self, text):
        """D·ª± ƒëo√°n spam d·ª±a tr√™n t·ª´ kh√≥a khi kh√¥ng c√≥ model AI"""
        temp_detector = SpamDetector.__new__(SpamDetector)
        temp_detector.device = torch.device('cpu')
        temp_detector.model = None 
        temp_detector.vocab = None
        return temp_detector.keyword_based_prediction(text) 